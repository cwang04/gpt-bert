method: bayes
metric:
  name: validation/perplexity      # change if your code logs a different key
  goal: minimize

command:
  - ${env}
  - ${interpreter}
  - -m
  - torch.distributed.run
  - --standalone
  - --nproc_per_node=4
  - ${program}
  - ${args}

program: pretraining/train_10m.py

parameters:
  # ---- fixed paths / constants from your run ----
  train_path:        {value: ./data/train_10M_tokenized.bin}
  valid_path:        {value: ./data/valid_10M_tokenized.bin}
  tokenizer_path:    {value: ./tokenizers/tokenizer_10M.json}
  config_file:       {value: ./configs/small.json}
  output_dir:        {value: ./model_checkpoints}
  name:              {value: hybrid_mlm_clm_10M}
  optimizer:         {value: lamb}
  seed:              {value: 42}
  ema_decay:         {value: 0.999}
  validate_every:    {value: 2500}
  save_every:        {value: 2500}
  local_batch_size:  {value: 4}
  hybrid_numerator: {value: 3}
  hybrid_denominator: {value: 4}
  max_steps: {value: 20000}
  seq_length: {value: 128}


  # ---- sweep these (most impact on perplexity) ----
  learning_rate:
    distribution: uniform
    min: 0.00001
    max: 0.0005 #from default adam lr

  global_batch_size:
    distribution: categorical
    values: [16, 32] 

  weight_decay:
    distribution: uniform
    min: 0.01
    max: 0.10

  warmup_proportion:
    distribution: uniform
    min: 0.04
    max: 0.10

  cooldown_proportion:
    distribution: uniform
    min: 0.04
    max: 0.05

  mask_p_start:
    distribution: uniform
    min: 0.2
    max: 0.4

  mask_p_end:
    distribution: uniform
    min: 0.1
    max: 0.2

  mask_random_p:
    distribution: uniform
    min: 0.05
    max: 0.2

  mask_keep_p:
    distribution: uniform
    min: 0.1
    max: 0.2
